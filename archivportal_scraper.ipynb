{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "Archivportal Scraper - Bürgerinitiativen"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Extracteur Archivportal-D - Bürgerinitiativen\n",
        "\n",
        "Ce notebook extrait automatiquement les initiatives citoyennes depuis archivportal-d.de\n",
        "\n",
        "**Colonnes extraites:**\n",
        "- Titre de l'initiative\n",
        "- Période (année ou laps de temps)\n",
        "- Lieu\n",
        "\n",
        "---\n",
        "\n",
        "## Mode d'emploi\n",
        "\n",
        "1. Clique sur **Exécution > Tout exécuter** (ou Ctrl+F9)\n",
        "2. Attends la fin (~2-3 minutes)\n",
        "3. Le fichier CSV se télécharge automatiquement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 1. Installation des dépendances (30 secondes)\n",
        "!pip install -q aiohttp beautifulsoup4 tqdm\n",
        "print(\"Dépendances installées !\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 2. Code du scraper\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import csv\n",
        "import hashlib\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional\n",
        "from urllib.parse import urljoin, quote\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"https://www.archivportal-d.de\"\n",
        "SEARCH_URL = f\"{BASE_URL}/objekte\"\n",
        "QUERY = \"Bürgerinitiativen\"\n",
        "ROWS_PER_PAGE = 100\n",
        "MAX_CONCURRENT = 20\n",
        "TIMEOUT = 30\n",
        "\n",
        "GERMAN_CITIES = {\n",
        "    'berlin', 'hamburg', 'münchen', 'munich', 'köln', 'cologne', 'frankfurt',\n",
        "    'stuttgart', 'düsseldorf', 'dortmund', 'essen', 'leipzig', 'bremen',\n",
        "    'dresden', 'hannover', 'nürnberg', 'duisburg', 'bochum', 'wuppertal',\n",
        "    'bielefeld', 'bonn', 'münster', 'karlsruhe', 'mannheim', 'augsburg',\n",
        "    'wiesbaden', 'gelsenkirchen', 'mönchengladbach', 'braunschweig', 'chemnitz',\n",
        "    'kiel', 'aachen', 'halle', 'magdeburg', 'freiburg', 'krefeld', 'lübeck',\n",
        "    'oberhausen', 'erfurt', 'mainz', 'rostock', 'kassel', 'hagen', 'hamm',\n",
        "    'saarbrücken', 'mülheim', 'potsdam', 'ludwigshafen', 'oldenburg', 'leverkusen',\n",
        "    'osnabrück', 'solingen', 'heidelberg', 'herne', 'neuss', 'darmstadt',\n",
        "    'paderborn', 'regensburg', 'ingolstadt', 'würzburg', 'wolfsburg', 'ulm',\n",
        "    'heilbronn', 'pforzheim', 'göttingen', 'bottrop', 'trier', 'recklinghausen',\n",
        "    'reutlingen', 'bremerhaven', 'koblenz', 'bergisch gladbach', 'jena',\n",
        "    'remscheid', 'erlangen', 'moers', 'siegen', 'hildesheim', 'salzgitter',\n",
        "    'dormagen', 'wertheim', 'aichelberg', 'wyhl', 'gorleben', 'brokdorf',\n",
        "    'kalkar', 'wackersdorf', 'grohnde', 'biblis', 'neckarwestheim',\n",
        "    'baden-württemberg', 'bayern', 'bavaria', 'brandenburg', 'hessen',\n",
        "    'mecklenburg-vorpommern', 'niedersachsen', 'nordrhein-westfalen', 'nrw',\n",
        "    'rheinland-pfalz', 'saarland', 'sachsen', 'sachsen-anhalt', 'schleswig-holstein',\n",
        "    'thüringen', 'rhein-kreis', 'schwarzwald', 'eifel', 'hunsrück', 'taunus',\n",
        "}\n",
        "\n",
        "@dataclass\n",
        "class Initiative:\n",
        "    titre: str\n",
        "    periode: str\n",
        "    lieu: str\n",
        "    url: str = \"\"\n",
        "    institution: str = \"\"\n",
        "\n",
        "    def to_dict(self):\n",
        "        return asdict(self)\n",
        "\n",
        "    def hash_key(self) -> str:\n",
        "        key = f\"{self.titre.lower().strip()}|{self.periode}|{self.lieu.lower().strip()}\"\n",
        "        return hashlib.md5(key.encode()).hexdigest()\n",
        "\n",
        "\n",
        "class ArchivportalScraper:\n",
        "    def __init__(self):\n",
        "        self.session: Optional[aiohttp.ClientSession] = None\n",
        "        self.results: list[Initiative] = []\n",
        "        self.seen_hashes: set[str] = set()\n",
        "        self.duplicates = 0\n",
        "        self.errors = 0\n",
        "        self.semaphore = asyncio.Semaphore(MAX_CONCURRENT)\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        timeout = aiohttp.ClientTimeout(total=TIMEOUT)\n",
        "        connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT)\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (compatible; ArchivScraper/1.0)',\n",
        "            'Accept': 'text/html',\n",
        "            'Accept-Encoding': 'gzip, deflate',\n",
        "        }\n",
        "        self.session = aiohttp.ClientSession(timeout=timeout, connector=connector, headers=headers)\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, *args):\n",
        "        if self.session:\n",
        "            await self.session.close()\n",
        "\n",
        "    async def fetch(self, url: str, retries: int = 3) -> Optional[str]:\n",
        "        async with self.semaphore:\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    async with self.session.get(url) as response:\n",
        "                        if response.status == 200:\n",
        "                            return await response.text()\n",
        "                        elif response.status == 429:\n",
        "                            await asyncio.sleep(2 ** attempt)\n",
        "                        else:\n",
        "                            self.errors += 1\n",
        "                            return None\n",
        "                except:\n",
        "                    await asyncio.sleep(1)\n",
        "            return None\n",
        "\n",
        "    async def get_total_results(self) -> int:\n",
        "        url = f\"{SEARCH_URL}?lang=en&query={quote(QUERY)}&offset=0&rows=1\"\n",
        "        html = await self.fetch(url)\n",
        "        if not html:\n",
        "            return 0\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        match = re.search(r'of\\s+([\\d,]+)', soup.get_text())\n",
        "        if match:\n",
        "            return int(match.group(1).replace(',', ''))\n",
        "        return 0\n",
        "\n",
        "    def extract_date(self, text: str) -> str:\n",
        "        if not text:\n",
        "            return \"Non spécifiée\"\n",
        "        text = text.strip()\n",
        "        match = re.search(r'(\\d{4})\\s*[-–]\\s*(\\d{4})', text)\n",
        "        if match:\n",
        "            return f\"{match.group(1)}-{match.group(2)}\"\n",
        "        match = re.search(r'(\\d{2}\\.\\d{2}\\.\\d{4})', text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        match = re.search(r'\\b(19\\d{2}|20[0-2]\\d)\\b', text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        if any(nd in text.lower() for nd in ['ohne datum', 'undatiert', 's.d.']):\n",
        "            return \"Non datée\"\n",
        "        return \"Non spécifiée\"\n",
        "\n",
        "    def extract_location(self, text: str, title: str = \"\") -> str:\n",
        "        combined = f\"{title} {text}\".lower()\n",
        "        for city in GERMAN_CITIES:\n",
        "            if city in combined:\n",
        "                pattern = r'\\b' + re.escape(city) + r'\\b'\n",
        "                if re.search(pattern, combined):\n",
        "                    return city.title()\n",
        "        full_text = f\"{title} {text}\"\n",
        "        for pattern in [r'\\bin\\s+([A-ZÄÖÜ][a-zäöüß]+)', r'\\bRegion\\s+([A-ZÄÖÜ][a-zäöüß]+)']:\n",
        "            match = re.search(pattern, full_text)\n",
        "            if match and match.group(1).lower() not in ['der', 'die', 'das', 'und']:\n",
        "                return match.group(1)\n",
        "        archive_match = re.search(r'(?:Stadtarchiv|Landesarchiv|Archiv)\\s+([A-ZÄÖÜ][a-zäöüß-]+)', text)\n",
        "        if archive_match:\n",
        "            return f\"(Archive: {archive_match.group(1)})\"\n",
        "        return \"Non spécifié\"\n",
        "\n",
        "    def extract_institution(self, text: str) -> str:\n",
        "        for pattern in [r'((?:Stadt|Landes|Bundes)?[Aa]rchiv[^,\\n]+)']:\n",
        "            match = re.search(pattern, text)\n",
        "            if match:\n",
        "                return match.group(1).strip()\n",
        "        return \"\"\n",
        "\n",
        "    def parse_list_item(self, item_html: str) -> Optional[Initiative]:\n",
        "        soup = BeautifulSoup(item_html, 'html.parser')\n",
        "        link = soup.find('a', href=re.compile(r'/item/'))\n",
        "        if not link:\n",
        "            return None\n",
        "        titre = link.get_text(strip=True)\n",
        "        url = urljoin(BASE_URL, link.get('href', ''))\n",
        "        full_text = soup.get_text(' ', strip=True)\n",
        "        meta_text = full_text.replace(titre, '', 1).strip()\n",
        "        return Initiative(\n",
        "            titre=titre,\n",
        "            periode=self.extract_date(meta_text),\n",
        "            lieu=self.extract_location(meta_text, titre),\n",
        "            url=url,\n",
        "            institution=self.extract_institution(meta_text)\n",
        "        )\n",
        "\n",
        "    async def parse_list_page(self, html: str) -> list[Initiative]:\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        results = []\n",
        "        for link in soup.find_all('a', href=re.compile(r'/item/')):\n",
        "            parent = link.find_parent(['li', 'div', 'article', 'tr'])\n",
        "            item_html = str(parent) if parent else str(link.parent) if link.parent else str(link)\n",
        "            initiative = self.parse_list_item(item_html)\n",
        "            if initiative:\n",
        "                results.append(initiative)\n",
        "        return results\n",
        "\n",
        "    def add_result(self, initiative: Initiative) -> bool:\n",
        "        key = initiative.hash_key()\n",
        "        if key in self.seen_hashes:\n",
        "            self.duplicates += 1\n",
        "            return False\n",
        "        self.seen_hashes.add(key)\n",
        "        self.results.append(initiative)\n",
        "        return True\n",
        "\n",
        "    async def scrape_all(self) -> list[Initiative]:\n",
        "        print(\"Récupération du nombre de résultats...\")\n",
        "        total = await self.get_total_results()\n",
        "        if total == 0:\n",
        "            print(\"Erreur: impossible de récupérer les résultats\")\n",
        "            return []\n",
        "        print(f\"{total} résultats trouvés\")\n",
        "        print(\"\\nExtraction en cours...\")\n",
        "\n",
        "        pages = (total + ROWS_PER_PAGE - 1) // ROWS_PER_PAGE\n",
        "        urls = [f\"{SEARCH_URL}?lang=en&query={quote(QUERY)}&offset={i * ROWS_PER_PAGE}&rows={ROWS_PER_PAGE}\" for i in range(pages)]\n",
        "\n",
        "        pbar = tqdm(total=total, desc=\"Progression\")\n",
        "\n",
        "        async def process_page(url: str):\n",
        "            html = await self.fetch(url)\n",
        "            if html:\n",
        "                items = await self.parse_list_page(html)\n",
        "                for item in items:\n",
        "                    if self.add_result(item):\n",
        "                        pbar.update(1)\n",
        "\n",
        "        for i in range(0, len(urls), 10):\n",
        "            batch = urls[i:i + 10]\n",
        "            await asyncio.gather(*[process_page(url) for url in batch])\n",
        "\n",
        "        pbar.close()\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Terminé: {len(self.results)} initiatives extraites\")\n",
        "        if self.duplicates > 0:\n",
        "            print(f\"Doublons ignorés: {self.duplicates}\")\n",
        "        if self.errors > 0:\n",
        "            print(f\"Erreurs: {self.errors}\")\n",
        "        print(f\"{'='*50}\")\n",
        "        return self.results\n",
        "\n",
        "print(\"Code chargé !\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 3. Lancer l'extraction (~2-3 minutes)\n",
        "async def run_scraper():\n",
        "    async with ArchivportalScraper() as scraper:\n",
        "        await scraper.scrape_all()\n",
        "        return scraper.results\n",
        "\n",
        "results = await run_scraper()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 4. Télécharger le fichier CSV\n",
        "from google.colab import files\n",
        "\n",
        "# Créer le CSV\n",
        "filename = \"burgerinitiativen.csv\"\n",
        "with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=['titre', 'periode', 'lieu', 'institution', 'url'])\n",
        "    writer.writeheader()\n",
        "    for init in results:\n",
        "        writer.writerow(init.to_dict())\n",
        "\n",
        "print(f\"Fichier créé: {filename}\")\n",
        "print(f\"Nombre de lignes: {len(results)}\")\n",
        "print(\"\\nTéléchargement automatique...\")\n",
        "\n",
        "# Télécharger automatiquement\n",
        "files.download(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@title 5. (Optionnel) Aperçu des données\n",
        "import pandas as pd\n",
        "df = pd.DataFrame([r.to_dict() for r in results])\n",
        "df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
